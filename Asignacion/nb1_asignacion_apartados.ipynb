{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rene-aum/Atlas/blob/main/Asignacion/nb1_asignacion_apartados.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7fbaca7",
      "metadata": {
        "id": "b7fbaca7"
      },
      "source": [
        "# Automatizaci√≥n del proceso de Asignaciones\n",
        "Por favor:\n",
        "1. Genera la hoja de sheets de salida en la carpeta correspondiente: https://drive.google.com/drive/folders/1AZuXU0aAEyzaA6OQbI_5F74Q568EoY-d\n",
        "\n",
        "2. Actualiza con cuidado el **d√≠a** üìÖ\n",
        "3. El n√∫mero de **corte diario** üî¢  \n",
        "4. La cosecha que est√°s asignando."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61115c0d",
      "metadata": {
        "cellView": "form",
        "id": "61115c0d"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, timedelta\n",
        "\n",
        "id_drive_pedidos = '1yVMEVT9zooZXOsiYHnZ1FZVGDZJQQ-sv'\n",
        "id_drive_ctes = '1UQvGtFjCLp47JrP7vuBRQpg9cfSAlJxH'\n",
        "id_drive_hist = '1zvW-Dxow9gz1Dnbpg_jO7my4wadDvJDW'\n",
        "id_drive_salidas = '1AZuXU0aAEyzaA6OQbI_5F74Q568EoY-d'\n",
        "id_sheets_tc2 = '1k8rguLeF1O33XCaVDxPiQ1C4SbxLDSIeqNcriYtsF-k'\n",
        "\n",
        "fh_salida = '2026-02-19' #@param{type:'date'}\n",
        "fh_salida_dt = datetime.strptime(fh_salida, '%Y-%m-%d')\n",
        "dia_salida = str(fh_salida_dt.day).zfill(2)\n",
        "mes_salida = str(fh_salida_dt.month).zfill(2)\n",
        "anio_salida = str(fh_salida_dt.year).zfill(4)\n",
        "corte = '3' #@param{type:'string'}[1,2,3]\n",
        "fh_de_asignacion = fh_salida_dt.strftime('%d-%m-%Y')\n",
        "\n",
        "cosecha = 'Cosecha Feb 26' #@param{type:'string'}\n",
        "\n",
        "actualizar_tc = 'S' #@param{type:'string'}['S','N']\n",
        "\n",
        "nb_carpeta_ctes_mes = f'{anio_salida}{mes_salida}'\n",
        "nb_ctes_csv = f'report_{anio_salida}{mes_salida}{dia_salida}_c{corte}.csv'\n",
        "nb_sheet_salida = f'Salidas {anio_salida}{mes_salida}{dia_salida}_c{corte}'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dia_salida, mes_salida, anio_salida, corte, nb_carpeta_ctes_mes, nb_ctes_csv, cosecha"
      ],
      "metadata": {
        "id": "n2frhAcZORwV"
      },
      "id": "n2frhAcZORwV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b4c7c853",
      "metadata": {
        "id": "b4c7c853"
      },
      "source": [
        "## Paqueter√≠as"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8defd04",
      "metadata": {
        "id": "a8defd04"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from_drive = True  # same flag you use everywhere\n",
        "\n",
        "if os.environ.get(\"HERMES_BOOTSTRAPPED\") != \"1\":\n",
        "    # ---------- GIT ON COLAB ONLY ----------\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "\n",
        "        git_token = userdata.get('gitToken')\n",
        "        git_user = userdata.get('gitUser')\n",
        "        git_url = f'https://{git_token}@github.com/rene-aum/Hermes.git'\n",
        "        branch_to_pull = 'dev'\n",
        "\n",
        "        os.chdir('/content')\n",
        "\n",
        "        if not os.path.isdir('Hermes'):\n",
        "            !git clone {git_url}\n",
        "\n",
        "        %cd Hermes\n",
        "        !git fetch origin {branch_to_pull}\n",
        "        !git checkout {branch_to_pull}\n",
        "        !git pull origin {branch_to_pull}\n",
        "\n",
        "        !pip install -r utils/src/requirements.txt\n",
        "        %cd Asignacion\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print('Running in other environment not colab probably!')\n",
        "\n",
        "    # ---------- DRIVE + SHEETS ----------\n",
        "    if from_drive:\n",
        "        from pydrive2.auth import GoogleAuth\n",
        "        from pydrive2.drive import GoogleDrive\n",
        "        from google.colab import auth\n",
        "        from oauth2client.client import GoogleCredentials\n",
        "        import gspread\n",
        "        from google.auth import default\n",
        "        from gspread_dataframe import set_with_dataframe\n",
        "        import gdown\n",
        "\n",
        "        auth.authenticate_user()\n",
        "        gauth = GoogleAuth()\n",
        "        gauth.credentials = GoogleCredentials.get_application_default()\n",
        "        drive = GoogleDrive(gauth)\n",
        "\n",
        "        creds, _ = default()\n",
        "        gc = gspread.authorize(creds)\n",
        "\n",
        "    os.environ[\"HERMES_BOOTSTRAPPED\"] = \"1\"\n",
        "else:\n",
        "    print(\"Bootstrap already done, assuming orchestrator ran it.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa4e4661",
      "metadata": {
        "id": "aa4e4661"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "sys.path.append('../..')\n",
        "from utils.utils import (get_dates_dataframe,\n",
        "                       add_year_week,\n",
        "                       custom_read,\n",
        "                       process_columns,\n",
        "                       remove_accents)\n",
        "\n",
        "from utils.drive_toolbox import(from_drive_to_local,\n",
        "                             get_last_modification_date_drive,\n",
        "                             create_sheets_in_drive_folder,\n",
        "                             update_sheets_in_drive_folder,\n",
        "                             read_from_google_sheets,\n",
        "                             list_file_ids_for_drive_folder,\n",
        "                             create_csv_file_in_drive_folder,\n",
        "                             write_csv_to_drive,\n",
        "                             read_csv_from_drive,\n",
        "                             append_dataframe_to_google_sheet_from_range)\n",
        "from utils.src.constants import (atlas_consumo_output_folder_id,\n",
        "                           consumo_sheets_ids_dict,\n",
        "                           folder_id_bauto_gabo,\n",
        "                           id_reporte_ventas,\n",
        "                           id_edas_referenciados,\n",
        "                           id_torre_de_control\n",
        "                           )\n",
        "\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18cb4889",
      "metadata": {
        "id": "18cb4889"
      },
      "outputs": [],
      "source": [
        "# EXTRAS\n",
        "\n",
        "# --------- BUSCAR EN SUBCARPETAS -------------------------------------------\n",
        "from googleapiclient.discovery import build\n",
        "creds, _ = default()\n",
        "\n",
        "servicedrive = build(\"drive\", \"v3\", credentials=creds)\n",
        "service_sheets = build(\"sheets\", \"v4\", credentials=creds)\n",
        "\n",
        "FOLDER_MIME = \"application/vnd.google-apps.folder\"\n",
        "\n",
        "def listar_archivos(folder_id, mime_types=None):\n",
        "    \"\"\"\n",
        "    folder_id: ID de la carpeta ra√≠z\n",
        "    mime_types: None | string | lista de strings\n",
        "    \"\"\"\n",
        "    if isinstance(mime_types, str):\n",
        "        mime_types = [mime_types]\n",
        "\n",
        "    resultados = {}\n",
        "\n",
        "    def recorrer(fid):\n",
        "        page_token = None\n",
        "        while True:\n",
        "            resp = servicedrive.files().list(\n",
        "                q=f\"'{fid}' in parents and trashed = false\",\n",
        "                fields=\"nextPageToken, files(id, name, mimeType)\",\n",
        "                pageToken=page_token,\n",
        "                supportsAllDrives=True,\n",
        "                includeItemsFromAllDrives=True\n",
        "            ).execute()\n",
        "\n",
        "            for f in resp.get(\"files\", []):\n",
        "                if f[\"mimeType\"] == FOLDER_MIME:\n",
        "                    recorrer(f[\"id\"])\n",
        "                else:\n",
        "                    if mime_types is None or f[\"mimeType\"] in mime_types:\n",
        "                        resultados[f[\"name\"]] = f[\"id\"]\n",
        "\n",
        "            page_token = resp.get(\"nextPageToken\")\n",
        "            if not page_token:\n",
        "                break\n",
        "\n",
        "    recorrer(folder_id)\n",
        "    return resultados\n",
        "\n",
        "\n",
        "# -------------- LEER CON ENCODING ------------------------------------------\n",
        "import io\n",
        "import pandas as pd\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "\n",
        "def read_csv_from_drive_v3(drive_service, file_id, **read_csv_kwargs):\n",
        "    request = drive_service.files().get_media(fileId=file_id)\n",
        "    fh = io.BytesIO()\n",
        "    downloader = MediaIoBaseDownload(fh, request)\n",
        "\n",
        "    done = False\n",
        "    while not done:\n",
        "        status, done = downloader.next_chunk()\n",
        "\n",
        "    fh.seek(0)\n",
        "    return pd.read_csv(fh, **read_csv_kwargs)\n",
        "\n",
        "\n",
        "# import io\n",
        "# def read_csv_from_drive2(drive, file_id, encoding=\"latin-1\", **read_csv_kwargs):\n",
        "#     f = drive.CreateFile({\"id\": file_id})\n",
        "#     f.FetchContent()\n",
        "#     b = f.content.getvalue()  # bytes\n",
        "#     return pd.read_csv(io.BytesIO(b), encoding=encoding, **read_csv_kwargs)\n",
        "\n",
        "# ------------- Todas las columnas ------------------------------------------\n",
        "pd.set_option('display.max_columns', 100)\n",
        "\n",
        "# ------------- IMPRIMIR CON COLORES ----------------------------------------\n",
        "class color:\n",
        "   PURPLE = '\\033[95m'\n",
        "   CYAN = '\\033[94m'\n",
        "   DARKCYAN = '\\033[36m'\n",
        "   BLUE = '\\033[94m'\n",
        "   GREEN = '\\033[92m'\n",
        "   YELLOW = '\\033[93m'\n",
        "   RED = '\\033[91m'\n",
        "   BOLD = '\\033[1m'\n",
        "   UNDERLINE = '\\033[4m'\n",
        "   END = '\\033[0m'\n",
        "import math\n",
        "from zoneinfo import ZoneInfo\n",
        "\n",
        "def borrar_hojas(spreadsheet_id, nb_hojas):\n",
        "    spreadsheet = service_sheets.spreadsheets().get(\n",
        "        spreadsheetId=spreadsheet_id\n",
        "    ).execute()\n",
        "\n",
        "    sheet_ids = []\n",
        "    for sheet in spreadsheet[\"sheets\"]:\n",
        "        if sheet[\"properties\"][\"title\"] in nb_hojas:\n",
        "            sheet_ids.append(sheet[\"properties\"][\"sheetId\"])\n",
        "\n",
        "    request = {\n",
        "        \"requests\": [\n",
        "            {\"deleteSheet\": {\"sheetId\": s_id}}\n",
        "            for s_id in sheet_ids\n",
        "            ]\n",
        "    }\n",
        "\n",
        "    service_sheets.spreadsheets().batchUpdate(\n",
        "        spreadsheetId=spreadsheet_id,\n",
        "        body=request\n",
        "    ).execute()\n",
        "    print(f\"{nb_hojas} eliminada(s)\")\n",
        "    return\n",
        "\n",
        "    print(\"Hoja no encontrada\")\n",
        "\n",
        "def crear_hojas_sheets(spreadsheet_id, nb_hojas, quitar_cuadricula = True, fila_congelada = 1):\n",
        "  \"\"\"\n",
        "  nb_hojas: lista con los nombres de las hojas a crear\n",
        "  quitar_cuadricula: True | False es para hacer invisibles los bordes/cuadr√≠cula de las celdas\n",
        "  \"\"\"\n",
        "\n",
        "  request = {\n",
        "      \"requests\": [\n",
        "          {\"addSheet\": {\"properties\": {\"title\": nombre,\n",
        "                                       'gridProperties': {'hideGridlines':quitar_cuadricula,\n",
        "                                                          'frozenRowCount':fila_congelada}\n",
        "                                       }\n",
        "                        }\n",
        "           }\n",
        "          for nombre in nb_hojas\n",
        "      ]\n",
        "  }\n",
        "\n",
        "  service_sheets.spreadsheets().batchUpdate(\n",
        "      spreadsheetId = id_sheets_salida,\n",
        "      body=request\n",
        "  ).execute()\n",
        "  print(f'{nb_hojas} creadas')\n",
        "\n",
        "\n",
        "def formato_hojas_sheets(sheets_id, nb_hojas, n_columnas, tamanio_letra = 11, letra = 'Source Serif 4', rgb_encabezado = [0.1, 0.3, 0.7], ):\n",
        "  spreadsheet = service_sheets.spreadsheets().get(\n",
        "      spreadsheetId=sheets_id\n",
        "  ).execute()\n",
        "\n",
        "  sheet_ids = []\n",
        "  for sheet in spreadsheet[\"sheets\"]:\n",
        "      if sheet[\"properties\"][\"title\"] in nb_hojas:\n",
        "          sheet_ids.append(sheet[\"properties\"][\"sheetId\"])\n",
        "\n",
        "  # requests de formato\n",
        "  requests = []\n",
        "  for sh_id in sheet_ids:\n",
        "    # A) Fuente para TODA la hoja\n",
        "    r_global = {\n",
        "        \"repeatCell\": {\n",
        "            \"range\": {\n",
        "                \"sheetId\": sh_id\n",
        "            },\n",
        "            \"cell\": {\n",
        "                \"userEnteredFormat\": {\n",
        "                    \"textFormat\": {\n",
        "                        \"fontFamily\": letra,\n",
        "                        \"fontSize\": tamanio_letra\n",
        "                    }\n",
        "                }\n",
        "            },\n",
        "            \"fields\": \"userEnteredFormat.textFormat(fontFamily,fontSize)\"\n",
        "        }\n",
        "    },\n",
        "\n",
        "    # B) Color solo en encabezado (fila 1)\n",
        "    r_encabezado = {\n",
        "        \"repeatCell\": {\n",
        "            \"range\": {\n",
        "                \"sheetId\": sh_id,\n",
        "                \"startRowIndex\": 0,\n",
        "                \"endRowIndex\": 1,\n",
        "                'startColumnIndex':0,\n",
        "                'endColumnIndex':n_columnas\n",
        "            },\n",
        "            \"cell\": {\n",
        "                \"userEnteredFormat\": {\n",
        "                    \"backgroundColor\": {\n",
        "                        \"red\": rgb_encabezado[0],\n",
        "                        \"green\": rgb_encabezado[1],\n",
        "                        \"blue\": rgb_encabezado[2]\n",
        "                    },\n",
        "                    \"textFormat\": {\n",
        "                        \"bold\": True,\n",
        "                        \"foregroundColor\": {\n",
        "                            \"red\": 1,\n",
        "                            \"green\": 1,\n",
        "                            \"blue\": 1\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            },\n",
        "            \"fields\": \"userEnteredFormat(backgroundColor,textFormat.bold,textFormat.foregroundColor)\"\n",
        "        }\n",
        "    }\n",
        "    r_anchoColumnas = {\n",
        "        \"autoResizeDimensions\": {\n",
        "            \"dimensions\": {\n",
        "                \"sheetId\": sh_id,\n",
        "                \"dimension\": \"COLUMNS\",\n",
        "                \"startIndex\": 0,\n",
        "                \"endIndex\": 20   # ajusta primeras 20 columnas\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    requests.append(r_global)\n",
        "    requests.append(r_encabezado)\n",
        "    requests.append(r_anchoColumnas)\n",
        "\n",
        "  service_sheets.spreadsheets().batchUpdate(\n",
        "      spreadsheetId=sheets_id,\n",
        "      body={\"requests\": requests}\n",
        "      ).execute()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98e8ab93",
      "metadata": {
        "id": "98e8ab93"
      },
      "source": [
        "## Leemos base de pedidos y nos quedamos con los nuevos desde el √∫ltimo corte"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9290ad02",
      "metadata": {
        "id": "9290ad02"
      },
      "outputs": [],
      "source": [
        "dict_PedidosSalesForce_id = listar_archivos(id_drive_pedidos)\n",
        "dict_PedidosSalesForce_id = {\n",
        "    x.split('_')[1]: v\n",
        "    for x, v in dict_PedidosSalesForce_id.items()\n",
        "}\n",
        "\n",
        "fto_fh = '%Y-%m-%d-%H'\n",
        "fhs_pedidos = list(dict_PedidosSalesForce_id.keys())\n",
        "fhs_pedidos = sorted([datetime.strptime(x, fto_fh) for x in fhs_pedidos], reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a30a18a2",
      "metadata": {
        "id": "a30a18a2"
      },
      "outputs": [],
      "source": [
        "ults_pdds = fhs_pedidos[0].strftime(fto_fh)\n",
        "pens_pdds = fhs_pedidos[1].strftime(fto_fh)\n",
        "print(f'Vamos a cargar pedidos entre {color.DARKCYAN} {fhs_pedidos[0]} y {fhs_pedidos[1]} {color.END} (ignorando minutos)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0560cf93",
      "metadata": {
        "id": "0560cf93"
      },
      "outputs": [],
      "source": [
        "pdds_sf_hoy = read_csv_from_drive_v3(servicedrive, dict_PedidosSalesForce_id[ults_pdds],encoding = 'latin1')\n",
        "pdds_sf_ayer = read_csv_from_drive_v3(servicedrive, dict_PedidosSalesForce_id[pens_pdds],encoding = 'latin1')\n",
        "columnas_sf = ['num_pedido','id_pedido','id_cuenta','nb_cuenta','id_vendedor','nb_comprador',\n",
        "                  'id_comprador','estatus','precio','fh_creacion','anticipo','estatus2','nb_producto','niv','desc','espacio','sku']\n",
        "pdds_sf_hoy.columns = pdds_sf_hoy.iloc[7].reset_index(drop=True)\n",
        "pdds_sf_hoy = pdds_sf_hoy.iloc[8:].reset_index(drop=True)\n",
        "pdds_sf_hoy = pdds_sf_hoy.dropna(axis=1, how=\"all\")\n",
        "pdds_sf_hoy.drop(columns= np.nan, inplace = True)\n",
        "if pdds_sf_hoy.shape[1]!=len(columnas_sf):  # 11 Feb 2026 agregamos columna SKU en el reporte de pedidos. Los previos no lo traer√°n, por lo que agregamos columna a mano\n",
        "  pdds_sf_hoy['sku'] = 'no_disponible'\n",
        "pdds_sf_hoy.columns = columnas_sf\n",
        "i_total = list(pdds_sf_hoy[pdds_sf_hoy['num_pedido']=='Total'].index)[0]\n",
        "pdds_sf_hoy = pdds_sf_hoy.iloc[:i_total]\n",
        "pdds_sf_hoy = pdds_sf_hoy[pd.to_datetime(pdds_sf_hoy['fh_creacion']) >= datetime(2025,7,1)].sort_values(by='num_pedido').reset_index(drop=True)\n",
        "\n",
        "pdds_sf_ayer.columns = pdds_sf_ayer.iloc[7].reset_index(drop=True)\n",
        "pdds_sf_ayer = pdds_sf_ayer.iloc[8:].reset_index(drop=True)\n",
        "pdds_sf_ayer = pdds_sf_ayer.dropna(axis=1, how=\"all\")\n",
        "pdds_sf_ayer.drop(columns= np.nan, inplace = True)\n",
        "if pdds_sf_ayer.shape[1]!=len(columnas_sf):  # 11 Feb 2026 agregamos columna SKU. Los previos no lo traer√°n, por lo que agregamos columna a mano\n",
        "  pdds_sf_ayer['sku'] = 'no_disponible'\n",
        "pdds_sf_ayer.columns = columnas_sf\n",
        "i_total = list(pdds_sf_ayer[pdds_sf_ayer['num_pedido']=='Total'].index)[0]\n",
        "pdds_sf_ayer = pdds_sf_ayer.iloc[:i_total]\n",
        "pdds_sf_ayer = pdds_sf_ayer[pd.to_datetime(pdds_sf_ayer['fh_creacion']) >= datetime(2025,7,1)].sort_values(by='num_pedido').reset_index(drop=True)\n",
        "\n",
        "pdds_sf = pdds_sf_hoy[~pdds_sf_hoy['num_pedido'].isin(pdds_sf_ayer['num_pedido'])].copy().reset_index(drop=True)\n",
        "\n",
        "display(pdds_sf.sample())\n",
        "print(f'{color.BOLD}{color.CYAN}Tenemos {pdds_sf.shape[0]} pedidos nuevos{color.END}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5336251",
      "metadata": {
        "id": "c5336251"
      },
      "outputs": [],
      "source": [
        "# CELDA DE VALIDACI√ìN\n",
        "fh_final_pdds, fh_inicial_pdds = fhs_pedidos[0].replace(hour=0), fhs_pedidos[1].replace(hour=0)\n",
        "fhs_creacion_ls = [datetime.strptime(x, '%d/%m/%Y') for x in pdds_sf['fh_creacion'].unique().tolist()]\n",
        "\n",
        "if not all([x <= fh_final_pdds and x >= fh_inicial_pdds for x in fhs_creacion_ls]):\n",
        "    print(f\"{color.RED}Hay pedidos con fechas fuera del rango de actualizaci√≥n{color.END}\")\n",
        "    raise SystemExit"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1fca19e",
      "metadata": {
        "id": "f1fca19e"
      },
      "source": [
        "## Pegamos datos de clientes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d81f6f3",
      "metadata": {
        "id": "7d81f6f3"
      },
      "outputs": [],
      "source": [
        "id_drive_ctes_mes = list_file_ids_for_drive_folder(drive, id_drive_ctes)[f'{nb_carpeta_ctes_mes}']\n",
        "archs_ctesnvos = list_file_ids_for_drive_folder(drive, id_drive_ctes_mes)\n",
        "\n",
        "ctes_nvos = read_csv_from_drive_v3(servicedrive, archs_ctesnvos[nb_ctes_csv], encoding = 'latin-1')\n",
        "print(f'{color.BOLD}{color.CYAN}Se carg√≥ base {nb_ctes_csv}{color.END}')\n",
        "# ctes_nvos['fh_actualizacion'] = arch_reciente[0]\n",
        "ctes_nvos.rename(columns = {'Id Comercio Externo':'id_am', 'Tel√©fono':'phone', 'Email':'email'}, inplace=True)\n",
        "\n",
        "ctes_nvos = ctes_nvos[~ctes_nvos['id_am'].isna()]\n",
        "ctes_nvos['id_am'] = ctes_nvos['id_am'].astype(int)\n",
        "ctes_nvos = ctes_nvos[['id_am','phone','email']]\n",
        "ctes_nvos['origen_arch'] = 'ctes_sf'\n",
        "ctes_nvos.sample()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "613cc9ed",
      "metadata": {
        "id": "613cc9ed"
      },
      "outputs": [],
      "source": [
        "# Aqui debo dar prioridad a la base AcClientes sobre la de andr√©s, porque hay registros que ah√≠ est√°n m√°s completos (comentario de Rene)\n",
        "ctes = read_from_google_sheets(gc, consumo_sheets_ids_dict['AcClientes'])\n",
        "ctes['origen_arch'] = 'ctes_ac'\n",
        "\n",
        "ctes = pd.concat([ctes, ctes_nvos])\n",
        "ctes = ctes.drop_duplicates('id_am',keep='first').reset_index(drop=True)\n",
        "ctes['phone'] = pd.to_numeric(ctes['phone'].fillna(0),errors='coerce').astype('Int64').astype(str).apply(lambda x: x[-10:])\n",
        "ctes['id_am'] = ctes['id_am'].astype(int)\n",
        "\n",
        "display(ctes.sample())\n",
        "print(f'{color.BOLD}{color.CYAN}Tenemos {ctes.shape[0]} clientes{color.END}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3425403",
      "metadata": {
        "id": "b3425403"
      },
      "outputs": [],
      "source": [
        "# Pegamos info de clientes a los nuevos pedidos\n",
        "pdds_sf['id_comprador'] = pdds_sf['id_comprador'].astype(int)\n",
        "pdds_sf1 = pdds_sf.merge(ctes[['id_am','email','phone','origen_arch']], how = 'left', left_on = 'id_comprador', right_on = 'id_am')\n",
        "pdds_sf1.sample(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bde6397b",
      "metadata": {
        "id": "bde6397b"
      },
      "outputs": [],
      "source": [
        "if not pdds_sf1[(pdds_sf1['email'].isna()) | (pdds_sf1['phone'].isna())].shape[0] == 0:\n",
        "    print(f\"{color.RED}Hay pedidos sin datos de comprador{color.END}\")\n",
        "    raise SystemExit"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1141c8c",
      "metadata": {
        "id": "b1141c8c"
      },
      "source": [
        "## Buscamos datos de pedidos nuevos en hist√≥rico y nos quedamos con los que cumplan definici√≥n de leads nuevos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "041cfbf4",
      "metadata": {
        "id": "041cfbf4"
      },
      "outputs": [],
      "source": [
        "id_hist = '1zvW-Dxow9gz1Dnbpg_jO7my4wadDvJDW'\n",
        "hist = list_file_ids_for_drive_folder(drive, id_hist)\n",
        "hist = [v for i,v in hist.items() if '_latest.csv' in i ][0]\n",
        "hist = read_csv_from_drive_v3(servicedrive, hist, encoding='latin-1' )\n",
        "\n",
        "hist['fecha de asignacion'] = pd.to_datetime(hist['fecha de asignacion'], format = '%Y-%m-%d')\n",
        "hist = hist.sort_values(by='fecha de asignacion', ascending=False)\n",
        "hist.sample(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "894b00bd",
      "metadata": {
        "id": "894b00bd"
      },
      "outputs": [],
      "source": [
        "hist_id = hist[['id comprador','id lead','estatus de lead']].copy().drop_duplicates('id comprador').dropna(subset='id comprador')\n",
        "hist_mail = hist[['mail comprador','id lead','estatus de lead']].copy().drop_duplicates('mail comprador').dropna(subset='mail comprador')\n",
        "hist_fon = hist[['telefono comprador','id lead','estatus de lead']].copy().drop_duplicates('telefono comprador').dropna(subset='telefono comprador')\n",
        "\n",
        "hist_id.columns = ['id_comprador','id_lead','estatus_lead']\n",
        "hist_mail.columns = ['email','id_lead','estatus_lead']\n",
        "hist_fon.columns = ['phone','id_lead','estatus_lead']\n",
        "\n",
        "pdds_sf1['id_comprador'] = pdds_sf1['id_comprador'].astype(str)\n",
        "pdds_sf1['phone'] = pd.to_numeric(pdds_sf1['phone'].astype(str).str[-10:],errors = 'coerce').astype('Int64')\n",
        "hist_fon['phone'] = pd.to_numeric(hist_fon['phone'].astype(str).str[-10:],errors='coerce').astype('Int64')\n",
        "pdds_sf2 = pdds_sf1.merge(hist_id[['id_comprador','id_lead','estatus_lead']], how = 'left', on = 'id_comprador', suffixes = ['','_conid'])\n",
        "pdds_sf2 = pdds_sf2.merge(hist_mail[['email','id_lead','estatus_lead']], how = 'left', on = 'email', suffixes = ['','_conemail'])\n",
        "pdds_sf2 = pdds_sf2.merge(hist_fon[['phone','id_lead','estatus_lead']], how = 'left', on = 'phone', suffixes = ['','_confon'])\n",
        "\n",
        "pdds_sf2['id_lead'] = np.where(pdds_sf2['id_lead'].notna(), pdds_sf2['id_lead'],\n",
        "                               np.where(pdds_sf2['id_lead_conemail'].notna(), pdds_sf2['id_lead_conemail'],\n",
        "                                        np.where(pdds_sf2['id_lead_confon'].notna(), pdds_sf2['id_lead_confon'],np.nan)))\n",
        "\n",
        "total_congruentes_incongruentes = len(pdds_sf2)\n",
        "cerrados = ['COMPRA EXITOSA ','COMPRA EXITOSA','CERRADO','NA']\n",
        "pdds_sf2['aux'] =((~pdds_sf2['estatus_lead'].fillna('NA').isin(cerrados) )*1 + (~pdds_sf2['estatus_lead_conemail'].fillna('NA').isin(cerrados))*1 +\n",
        "                  (~pdds_sf2['estatus_lead_confon'].fillna('NA').isin(cerrados))*1)\n",
        "pdds_stts_incongruente = pdds_sf2[pdds_sf2['aux'].between(1,2)].copy()\n",
        "pdds_sf2 = pdds_sf2[pdds_sf2['aux'].isin([0,3])].copy()\n",
        "\n",
        "pdds_sf2['estatus_lead'] = np.where(pdds_sf2['estatus_lead'].notna(), pdds_sf2['estatus_lead'],\n",
        "                               np.where(pdds_sf2['estatus_lead_conemail'].notna(), pdds_sf2['estatus_lead_conemail'],\n",
        "                                        np.where(pdds_sf2['estatus_lead_confon'].notna(), pdds_sf2['estatus_lead_confon'],np.nan)))\n",
        "\n",
        "nvos_leads = ( (pdds_sf2['id_lead'].isna()) | (pdds_sf2['estatus_lead'].isin(cerrados) ) )\n",
        "\n",
        "leads_ok = pdds_sf2[~nvos_leads].copy()\n",
        "leads_ok = pd.concat([leads_ok, pdds_stts_incongruente])\n",
        "\n",
        "# leads_nvos = pdds_sf2.copy()\n",
        "# leads_nvos = leads_nvos[~leads_nvos['num_pedido'].isin(pedidos_multicontacto['num_pedido'].unique())]\n",
        "leads_nvos = pdds_sf2[nvos_leads].copy()\n",
        "\n",
        "print(len(leads_ok), len(leads_nvos), total_congruentes_incongruentes)\n",
        "display(pdds_stts_incongruente)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e3323a8",
      "metadata": {
        "id": "6e3323a8"
      },
      "outputs": [],
      "source": [
        "if not len(leads_ok) + len(leads_nvos) == total_congruentes_incongruentes:\n",
        "    print(f\"{color.RED}El total de pedidos que requieren leads, considerando los incongruentes, no cuadra con el total de pedidos iniciales{color.END}\")\n",
        "    print(f\"{color.BOLD}{color.CYAN}La clasificaci√≥n de leads ok (pedidos que no requieren lead nuevo) y leads nuevos est√° perdiendo alguno(s) de los pedidos con los que iniciamos {color.END}\")\n",
        "    raise SystemExit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70dd82c2",
      "metadata": {
        "id": "70dd82c2"
      },
      "outputs": [],
      "source": [
        "leads_nvos = leads_nvos[['id_comprador','espacio','phone','email','nb_comprador']].drop_duplicates('id_comprador', keep='last').reset_index(drop=True)\n",
        "leads_nvos['espacio'] = leads_nvos['espacio'].replace({'Metr√≥poli Patriotismo':'patriotismo','Samara Sat√©lite':'samara','Reforma 510':'torre'})\n",
        "print(f'{color.BOLD}{color.CYAN} Tenemos {color.UNDERLINE}{len(leads_nvos)} leads nuevos{color.END}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35a5fc31",
      "metadata": {
        "id": "35a5fc31"
      },
      "source": [
        "## Asignaciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16e46720",
      "metadata": {
        "id": "16e46720"
      },
      "outputs": [],
      "source": [
        "# Leemos cat√°logo de asesores\n",
        "id_asesores = '1xQkepXoIoEHNgdYUaLAT7FtDnuzYo1qb'\n",
        "cat_as = list_file_ids_for_drive_folder(drive, id_asesores)['AsesoresEspacio']\n",
        "centros = ['torre','samara','patriotismo','celula_credito']\n",
        "assrs_actvs = {}\n",
        "for c in centros:\n",
        "  df = read_from_google_sheets(gc,cat_as,c)\n",
        "  df['espacio'] = c\n",
        "  df = df[df.activo==1].drop_duplicates(['asesor','espacio'])\n",
        "  assrs_actvs[c] = df\n",
        "assrs_actvs = pd.concat(assrs_actvs.values())\n",
        "assrs_actvs = assrs_actvs.reset_index(drop=True)\n",
        "assrs_actvs['asesor'] = assrs_actvs['asesor'].str.lower().str.strip()\n",
        "\n",
        "# N√∫mero de asesores activos por espacio\n",
        "num_assrs = {c: len(assrs_actvs[assrs_actvs.espacio==c]) for c in assrs_actvs.espacio.unique()}\n",
        "num_assrs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "028957cc",
      "metadata": {
        "id": "028957cc"
      },
      "outputs": [],
      "source": [
        "# Leads activos por asesor\n",
        "dicc_espacios = {'Metr√É¬≥poli Patriotismo':'patriotismo','Samara Sat√É¬©lite':'samara','Reforma 510':'torre'}\n",
        "\n",
        "leads_asesor_espacio = hist[~hist['estatus de lead'].isin(['COMPRA EXITOSA ','COMPRA EXITOSA', 'CERRADO'])].groupby(['asesor espacio','espacio automarket']).agg({'id lead':'nunique'}).reset_index()\n",
        "leads_asesor_espacio.columns = ['asesor','espacio','leads']\n",
        "leads_asesor_espacio['asesor'] = leads_asesor_espacio['asesor'].str.lower().str.strip()\n",
        "leads_asesor_espacio['espacio'] = leads_asesor_espacio['espacio'].replace(dicc_espacios)\n",
        "leads_asesor_espacio = leads_asesor_espacio[~leads_asesor_espacio['asesor'].str.strip().isin(['PRUEBA','prueba','#N/A()','#n/a ()'])]\n",
        "leads_asesor_espacio = leads_asesor_espacio.groupby(['asesor','espacio']).agg({'leads':'sum'}).reset_index()\n",
        "\n",
        "\n",
        "leads_asesor_cred = hist[~hist['estatus de lead'].isin(['COMPRA EXITOSA ','COMPRA EXITOSA', 'CERRADO'])].groupby(['asesor credito']).agg({'id lead':'nunique'}).reset_index()\n",
        "leads_asesor_cred.columns = ['asesor','leads']\n",
        "leads_asesor_cred['asesor'] = leads_asesor_cred['asesor'].str.lower().str.strip()\n",
        "leads_asesor_cred['espacio'] = 'celula_credito'\n",
        "leads_asesor_cred = leads_asesor_cred[~leads_asesor_cred['asesor'].str.strip().isin(['PRUEBA', 'prueba', '#N/A()', '#n/a ()', '', ' '])]\n",
        "leads_asesor_cred = leads_asesor_cred.groupby(['asesor','espacio']).agg({'leads':'sum'}).reset_index()\n",
        "\n",
        "leads_asesor = pd.concat([leads_asesor_espacio, leads_asesor_cred])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2748a667",
      "metadata": {
        "id": "2748a667"
      },
      "outputs": [],
      "source": [
        "assrs_actvs_leads = assrs_actvs.merge(leads_asesor, how = 'left', on = ['asesor','espacio'])\n",
        "assrs_actvs_leads = assrs_actvs_leads.sort_values(['espacio','leads']).reset_index(drop=True)\n",
        "\n",
        "assrs_actvs_leads['llave'] = assrs_actvs_leads.groupby('espacio').cumcount()\n",
        "assrs_actvs_leads['leads'] = assrs_actvs_leads['leads'].fillna(0)\n",
        "\n",
        "leads_asesor_cred = assrs_actvs_leads[assrs_actvs_leads['espacio'] == 'celula_credito'].rename(columns = {'asesor':'asesor credito'})\n",
        "leads_asesor_esp = assrs_actvs_leads[assrs_actvs_leads['espacio'] != 'celula_credito'].rename(columns = {'asesor':'asesor espacio'})\n",
        "display(leads_asesor_cred)\n",
        "display(leads_asesor_esp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab0e6d08",
      "metadata": {
        "id": "ab0e6d08"
      },
      "outputs": [],
      "source": [
        "# Vamos a procesar distinto los leads que ya tuvieron gesti√≥n en alg√∫n espacio. Los seleccionamos con inner merge vs hist√≥rico\n",
        "leads_nvos_comprprevio = leads_nvos.copy().rename(columns = {'espacio':'espacio sf'}) #espacio sf es el nuevo espacio que asigna salesforce\n",
        "leads_nvos_comprprevio = leads_nvos_comprprevio.merge(\n",
        "    hist[['id comprador','espacio automarket','asesor espacio']].rename(\n",
        "    columns = {'espacio automarket':'espacio previo', 'asesor espacio':'asesor espacio previo'}\n",
        "    ).drop_duplicates(\n",
        "        'id comprador',keep='first'),\n",
        "                                        how = 'inner', left_on = 'id_comprador', right_on = 'id comprador').drop(columns = ['id comprador'])\n",
        "\n",
        "leads_nvos_comprprevio['espacio previo'] = leads_nvos_comprprevio['espacio previo'].replace(dicc_espacios)\n",
        "\n",
        "# Mergeamos por el espacio que ten√≠a en su lead anterior (espacio 1) con cat√°logo de asesores y nos quedamos con el primer asesor que cruce y no haya tenido antes (columna aux)\n",
        "leads_nvos_comprprevio = leads_nvos_comprprevio.merge(leads_asesor_esp.rename(columns = {'asesor espacio':'asesor espacio nvo'}), how = 'left', left_on = 'espacio previo', right_on = 'espacio')\n",
        "leads_nvos_comprprevio['aux'] = (leads_nvos_comprprevio['asesor espacio previo'].str.lower().str.strip() == leads_nvos_comprprevio['asesor espacio nvo'].str.lower().str.strip())*1\n",
        "leads_nvos_comprprevio = leads_nvos_comprprevio.sort_values(by='aux', ascending=True)\n",
        "leads_nvos_comprprevio = leads_nvos_comprprevio.drop_duplicates(['id_comprador'], keep = 'first').reset_index(drop=True)\n",
        "leads_nvos_comprprevio = leads_nvos_comprprevio.drop(columns = ['espacio sf','espacio previo','asesor espacio previo','activo','leads','llave','aux']).rename(columns = {'asesor espacio nvo':'asesor espacio'})\n",
        "display(leads_nvos_comprprevio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "575d8ad3",
      "metadata": {
        "id": "575d8ad3"
      },
      "outputs": [],
      "source": [
        "leads_nvos = leads_nvos[~leads_nvos['id_comprador'].isin(leads_nvos_comprprevio['id_comprador'].unique())]\n",
        "leads_nvos['llave_esp'] = leads_nvos.groupby('espacio').cumcount() % leads_nvos['espacio'].map(num_assrs)\n",
        "leads_nvos = leads_nvos.merge(leads_asesor_esp, how='left', left_on = ['espacio','llave_esp'], right_on = ['espacio','llave'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f2adca5",
      "metadata": {
        "id": "5f2adca5"
      },
      "outputs": [],
      "source": [
        "salida_leads = pd.concat([leads_nvos, leads_nvos_comprprevio]).sort_values(by='id_comprador').reset_index(drop=True)\n",
        "\n",
        "salida_leads['llave_celcred'] = salida_leads.index % num_assrs['celula_credito']\n",
        "salida_leads = salida_leads.merge(leads_asesor_cred, how = 'left', left_on = 'llave_celcred', right_on = 'llave', suffixes = ['','_cred'])\n",
        "leads_nvos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f8b956a",
      "metadata": {
        "id": "5f8b956a"
      },
      "outputs": [],
      "source": [
        "hist['conteo_leads'] = hist['id lead'].str.replace('|','0').str[-6:].astype(int)\n",
        "ultimo_lead = hist['conteo_leads'].max()\n",
        "ultimo_lead"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9711a19d",
      "metadata": {
        "id": "9711a19d"
      },
      "outputs": [],
      "source": [
        "salida_leads = salida_leads.reset_index(drop=True)\n",
        "salida_leads = salida_leads.reset_index()\n",
        "salida_leads['index'] = salida_leads['index']+1\n",
        "salida_leads['id lead'] = salida_leads['index'] + ultimo_lead\n",
        "salida_leads['id lead'] = 'LAA-' + salida_leads['id lead'].astype(str).str.zfill(6)\n",
        "salida_leads.drop(columns = ['llave','index','activo','leads'], inplace = True)\n",
        "salida_leads['folio bauto tc'] = 'x'\n",
        "salida_leads['origen automarket'] = 'Apartado'\n",
        "salida_leads['cosecha'] = cosecha\n",
        "salida_leads['fecha de asignacion'] = fh_de_asignacion\n",
        "\n",
        "salida_leads.rename(columns = {'id_comprador':'id comprador','espacio':'espacio automarket','phone':'telefono comprador','email':'mail comprador',\n",
        "                               'nb_comprador':'nombre comprador'},inplace=True)\n",
        "salida_leads = salida_leads[['id lead','origen automarket','cosecha','id comprador','folio bauto tc',\n",
        "                             'nombre comprador','mail comprador','telefono comprador','asesor credito','espacio automarket','asesor espacio','fecha de asignacion']]\n",
        "salida_leads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e057a71a",
      "metadata": {
        "id": "e057a71a"
      },
      "outputs": [],
      "source": [
        "id_l_nvos = salida_leads['id lead'].unique()\n",
        "\n",
        "if not hist[hist['id lead'].isin(id_l_nvos)].shape[0] == 0:\n",
        "    print(f\"{color.RED}Se est√°n duplicando id leads respecto a los que ya exist√≠an en la torre de control{color.END}\")\n",
        "    raise SystemExit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20df3816",
      "metadata": {
        "id": "20df3816"
      },
      "outputs": [],
      "source": [
        "asgns_as_es = salida_leads.groupby(['asesor espacio','espacio automarket']).agg(nuevos = ('id comprador','nunique')).reset_index()\n",
        "asgns_as_cr = salida_leads.groupby(['asesor credito']).agg(nuevos = ('id comprador','nunique')).reset_index().rename(columns = {'asesor credito':'asesor espacio'})\n",
        "asgns_as_cr['espacio automarket'] = 'celula_credito'\n",
        "\n",
        "asgns_as = pd.concat([asgns_as_es, asgns_as_cr])\n",
        "asgns_as['asesor espacio'] = asgns_as['asesor espacio'].str.lower()\n",
        "asgns_as['espacio automarket'] = asgns_as['espacio automarket'].replace({'Metr√≥poli Patriotismo':'patriotismo','Reforma 510':'torre','Samara Sat√©lite':'samara'})\n",
        "\n",
        "assrs_actvs_leads.columns = ['asesor espacio','activo','espacio automarket','leads','llave']\n",
        "asgns_res = assrs_actvs_leads.merge(asgns_as, how = 'left', on = ['asesor espacio','espacio automarket'])\n",
        "\n",
        "asgns_res['leads_nuevos'] = asgns_res['leads'] + asgns_res['nuevos']\n",
        "asgns_res"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dd523c2",
      "metadata": {
        "id": "0dd523c2"
      },
      "source": [
        "## Contact Center"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a8304a5",
      "metadata": {
        "id": "8a8304a5"
      },
      "outputs": [],
      "source": [
        "id_comprador_leadsnvos = salida_leads['id comprador'].unique()\n",
        "leads_cc = hist[hist['estatus de lead']=='Contact Center']['id comprador'].unique()\n",
        "\n",
        "cc = pdds_sf2[pdds_sf2['id_comprador'].isin(list(id_comprador_leadsnvos) + list(leads_cc))].reset_index(drop=True)\n",
        "cc = cc[['num_pedido','id_comprador','id_vendedor','fh_creacion']]\n",
        "datos_nvosleads_previoscc = pd.concat([salida_leads,hist]).reset_index(drop=True)  #.drop_duplicates('id')\n",
        "cols = ['id lead','id comprador','fecha de asignacion','origen automarket','asesor espacio','espacio automarket']\n",
        "datos_nvosleads_previoscc = datos_nvosleads_previoscc[cols].drop_duplicates('id comprador')           # Deduplico y priorizo los que estamos asignando primero y despu√©s los hist√≥ricos empezando por los m√°s recientes (hist est√° ordenado as√≠)\n",
        "salida_cc = cc.merge(datos_nvosleads_previoscc, how='left',left_on='id_comprador',right_on = 'id comprador')\n",
        "salida_cc['fecha de asignacion'] = salida_cc['fecha de asignacion'].astype(str).str[:10]\n",
        "salida_cc['num_pedido'] = salida_cc['num_pedido'].astype(int)\n",
        "salida_cc\n",
        "# FALTA CRUZAR CONTRA EL HIST√ìRICO PARA LA INFO DE LOS QUE YA ESTABAN PERO CON ESTATUS DE CONTACT CENTER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1a7ff2d",
      "metadata": {
        "id": "f1a7ff2d"
      },
      "outputs": [],
      "source": [
        "salida_cc['id_vendedor'] = salida_cc['id_vendedor'].astype(str)\n",
        "ctes['id_am'] = ctes['id_am'].astype(str)\n",
        "\n",
        "salida_cc = salida_cc.merge(ctes[['id_am','billing_firstname','email','phone']], how = 'left', left_on = 'id_vendedor', right_on = 'id_am')\n",
        "\n",
        "salida_cc['fecha de asignacion cc'] = datetime.today().strftime('%Y-%m-%d')\n",
        "salida_cc = salida_cc[['id lead','fecha de asignacion','fecha de asignacion cc','origen automarket','asesor espacio','espacio automarket',\n",
        "                       'id_comprador','num_pedido','fh_creacion','id_vendedor','billing_firstname','email','phone']]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca7dd431",
      "metadata": {
        "id": "ca7dd431"
      },
      "source": [
        "## Guardado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e6b3d95",
      "metadata": {
        "id": "1e6b3d95"
      },
      "outputs": [],
      "source": [
        "dicc_espacios2 = {'Metr√É¬≥poli Patriotismo': 'Metr√≥poli Patriotismo','Samara Sat√É¬©lite': 'Samara Sat√©lite'}\n",
        "\n",
        "salida_leads['espacio automarket'] = salida_leads['espacio automarket'].replace({'torre':'Reforma 510','patriotismo':'Metr√≥poli Patriotismo','samara':'Samara Sat√©lite'})\n",
        "salida_cc['espacio automarket'] = salida_cc['espacio automarket'].replace(dicc_espacios2).replace({'torre':'Reforma 510','patriotismo':'Metr√≥poli Patriotismo','samara':'Samara Sat√©lite'})\n",
        "\n",
        "salida_leads['asesor credito'] = salida_leads['asesor credito'].str.title()\n",
        "salida_leads['asesor espacio'] = salida_leads['asesor espacio'].str.title()\n",
        "salida_cc['asesor espacio'] = salida_cc['asesor espacio'].str.title()\n",
        "\n",
        "salida_leads['estatus de lead'] = 'Contact Center'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos la hoja de sheets de cero\n",
        "\n",
        "ahora_dt = datetime.now(ZoneInfo(\"America/Mexico_City\")).strftime('%d-%m-%Y %H:%M')\n",
        "nb_sheets_salida = f'Salida {ahora_dt}'\n",
        "nb_hojas = ['asignacion_apartados','salida_cc']\n",
        "\n",
        "id_folder_mes_salida = list_file_ids_for_drive_folder(drive, id_drive_salidas)[f'{anio_salida}{mes_salida}']\n",
        "create_sheets_in_drive_folder(gc, nb_sheets_salida, id_folder_mes_salida)\n",
        "id_sheets_salida = listar_archivos(id_folder_mes_salida, mime_types='application/vnd.google-apps.spreadsheet')[nb_sheets_salida]\n",
        "\n",
        "# Creamos hojas y eliminamos la hoja 1.\n",
        "# Si esto da error, muy probablemente es porque ya hay un archivo con el mismo nombre en la carpeta. -----> Revisa la carpeta de drive.\n",
        "crear_hojas_sheets(id_sheets_salida, nb_hojas)\n",
        "borrar_hojas(id_sheets_salida,['Hoja 1'])"
      ],
      "metadata": {
        "id": "G9Ue4L6_E2Hw"
      },
      "id": "G9Ue4L6_E2Hw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardamos la salida del proceso de asignaci√≥n en hojas de respaldo\n",
        "\n",
        "hoja_df = {'asignacion_apartados': salida_leads, 'salida_cc':salida_cc}\n",
        "for hoja, df in hoja_df.items():\n",
        "  update_sheets_in_drive_folder(gc, id_sheets_salida, hoja, df)\n",
        "  formato_hojas_sheets(id_sheets_salida, [hoja], n_columnas = df.shape[1], letra = 'Source Serif 4' )\n",
        "\n",
        "print(f'Las salidas se generaron y se guardaron en https://docs.google.com/spreadsheets/d/{id_sheets_salida}')"
      ],
      "metadata": {
        "id": "LoyMo6NZFwfV"
      },
      "id": "LoyMo6NZFwfV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tomamos fotos a torre de control y creamos hojas correspondientes para depositarlas en reporte de asignacion\n",
        "ahora_dt = datetime.now(ZoneInfo(\"America/Mexico_City\")).strftime('%d-%m-%Y %H:%M')\n",
        "\n",
        "nb_foto_as = f'FotoAsig_TC_{ahora_dt}'\n",
        "asig_foto = read_from_google_sheets(gc, id_sheets_tc2, sheetname='asignacion')\n",
        "\n",
        "nb_foto_cc = f'FotoCC_TC_{ahora_dt}'\n",
        "cc_foto = read_from_google_sheets(gc, id_sheets_tc2, sheetname='contact center')\n",
        "\n",
        "crear_hojas_sheets(id_sheets_salida, [nb_foto_as, nb_foto_cc])\n",
        "\n",
        "# ========================Asignaciones ==========================\n",
        "# guardamos respaldo\n",
        "update_sheets_in_drive_folder(gc, id_sheets_salida, nb_foto_as, asig_foto)\n",
        "formato_hojas_sheets(id_sheets_salida, [nb_foto_as], n_columnas = asig_foto.shape[1], letra = 'Source Serif 4' )\n",
        "# actualizamos si aplica\n",
        "if actualizar_tc == 'S':\n",
        "  append_dataframe_to_google_sheet_from_range(gc, id_sheets_tc2, 'asignacion', salida_leads)\n",
        "else:\n",
        "  print('No se actualiz√≥ la hoja de asignaci√≥n en torre de control')\n",
        "# ===============================================================\n",
        "\n",
        "# ========================Contact Center ==========================\n",
        "# guardamos respaldo\n",
        "update_sheets_in_drive_folder(gc, id_sheets_salida, nb_foto_cc, cc_foto)\n",
        "formato_hojas_sheets(id_sheets_salida, [nb_foto_cc], n_columnas = cc_foto.shape[1], letra = 'Source Serif 4' )\n",
        "# actualizamos si aplica\n",
        "if actualizar_tc == 'S':\n",
        "  append_dataframe_to_google_sheet_from_range(gc, id_sheets_tc2, 'contact center', salida_cc)\n",
        "else:\n",
        "  print('No se actualiz√≥ la hoja de contact center en torre de control')\n",
        "# ==============================================================="
      ],
      "metadata": {
        "id": "-447paN1GTjr"
      },
      "id": "-447paN1GTjr",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "b4c7c853",
        "98e8ab93",
        "f1fca19e",
        "b1141c8c",
        "35a5fc31",
        "0dd523c2"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}